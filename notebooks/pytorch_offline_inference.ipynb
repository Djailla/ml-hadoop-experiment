{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "from ml_hadoop_experiment.pytorch.spark_inference import with_inference_column, with_inference_column_and_preprocessing\n",
    "from ml_hadoop_experiment.common.spark_inference import SerializableObj, artifact_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our model is a very simple neural network which hidden size is configurable.\n",
    "Your model will be broadcasted to executors at some point and thus must be serializable. For that reason,\n",
    "its definition must be in a Python module of your environment (and not in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_hadoop_experiment.pytorch.fixtures.test_models import ToyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyModel(\n",
       "  (hidden1): Linear(in_features=2, out_features=500, bias=True)\n",
       "  (hidden2): Linear(in_features=500, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter: hidden size\n",
    "model = ToyModel(500)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your dataset used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_spark_session():\n",
    "    if \"SPARK_HOME\" in os.environ.keys():\n",
    "        del os.environ[\"SPARK_HOME\"]\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\n",
    "    SparkSession.builder._options = {}\n",
    "    ssb = SparkSession.builder.master(\"local[1]\").config(\"spark.submit.deployMode\", \"client\")\n",
    "    ss = ssb.getOrCreate()\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session with create_remote_spark_session to run on yarn cluster\n",
    "# We create a local spark session here only for demonstration \n",
    "ss = create_local_spark_session(\"Inference with PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this example notebook, our dataset is composed of two features. Examples are randomly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = np.random.random([n_examples])\n",
    "feature2 = np.random.random([n_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[feature1: double, feature2: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.createDataFrame(\n",
    "    list(zip(feature1.tolist(), feature2.tolist())),\n",
    "    [\"feature1\", \"feature2\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure your model is serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is going to be broadcasted to executors so it must be serializable. We provide the wrapper SerializableObj that guarantee that your model is serializable. Simply define and import the function that loads your model and provide it to SerializableObj along with any parameters that you would need to load your model. Again, it is important that your function lies in a Python module of your environment (and not in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function thx.pytorch.fixtures.test_models.load_toy_model(hidden_size:int) -> thx.pytorch.fixtures.test_models.ToyModel>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_hadoop_experiment.pytorch.fixtures.test_models import load_toy_model\n",
    "load_toy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a serializable model by simply providing your the function and parameters to load our model\n",
    "serializable_model = SerializableObj(ss, load_toy_model, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the inference function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function describe how to call our model and how to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1st parameter is a list of artifacts that we need to run inference.\n",
    "# In our example, this is simply our model. It could be a tokenizer + model for example.\n",
    "\n",
    "# The 2nd parameter is a tuple of pandas Series, one series per input features.\n",
    "# So in our example, it is a tuple of two pandas Series.\n",
    "\n",
    "# The 3rd parameter is the device name (\"cpu\", \"cuda:0\", \"cuda:1\" ...). When running on CPU-only machines,\n",
    "# the device is \"cpu\". When running on GPU machines, the device is \"cuda:0\" or \"cuda:1\" or ...\n",
    "# depending on the number of GPUs available on the machine and the number of task per GPU machine.\n",
    "# Tasks are uniformly distributed on all GPUs.\n",
    "\n",
    "# The output must be a pandas series containing all your outputs. In our example, each element of the\n",
    "# pandas series is a list of 10 doubles (output of the model)\n",
    "def inference_fn(\n",
    "    artifacts: artifact_type, features: Tuple[pd.Series, ...], device: str\n",
    ") -> Tuple[pd.Series, ...]:\n",
    "    model = artifacts\n",
    "    model.to(device)\n",
    "    feature1, feature2 = features\n",
    "    feature_1_as_tensor = torch.Tensor(feature1.to_list())\n",
    "    feature_2_as_tensor = torch.Tensor(feature2.to_list())\n",
    "    results = model(feature_1_as_tensor, feature_2_as_tensor)\n",
    "    return pd.Series(results.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions = with_inference_column(\n",
    "    df=df,\n",
    "    artifacts=serializable_model,\n",
    "    input_cols=[\"feature1\", \"feature2\"],\n",
    "    inference_fn=inference_fn,\n",
    "    # Our model output is an array of doubles\n",
    "    output_type=ArrayType(DoubleType()),\n",
    "    batch_size=50,\n",
    "    output_col=\"predictions\",\n",
    "    num_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g.racic/offline_inference/lib/python3.6/site-packages/pyarrow/util.py:46: FutureWarning: pyarrow.open_stream is deprecated as of 0.17.0, please use pyarrow.ipc.open_stream instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pdf = df_with_predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.585711</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>[0.14496324956417084, 0.06333291530609131, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.260878</td>\n",
       "      <td>0.160317</td>\n",
       "      <td>[0.11619735509157181, 0.07071886956691742, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979533</td>\n",
       "      <td>0.947563</td>\n",
       "      <td>[0.1776239275932312, 0.052888333797454834, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212702</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>[0.11229074001312256, 0.07178390026092529, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098237</td>\n",
       "      <td>0.062555</td>\n",
       "      <td>[0.10662975162267685, 0.07581344991922379, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.826296</td>\n",
       "      <td>0.181083</td>\n",
       "      <td>[0.1443796455860138, 0.052474234253168106, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.206053</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>[0.13214512169361115, 0.07988481223583221, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.887212</td>\n",
       "      <td>[0.15844134986400604, 0.06172007694840431, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.089348</td>\n",
       "      <td>0.853725</td>\n",
       "      <td>[0.12560969591140747, 0.0846186950802803, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.124021</td>\n",
       "      <td>0.197372</td>\n",
       "      <td>[0.11106044799089432, 0.07632377743721008, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2                                        predictions\n",
       "0    0.585711  0.622785  [0.14496324956417084, 0.06333291530609131, 0.0...\n",
       "1    0.260878  0.160317  [0.11619735509157181, 0.07071886956691742, 0.0...\n",
       "2    0.979533  0.947563  [0.1776239275932312, 0.052888333797454834, 0.0...\n",
       "3    0.212702  0.091959  [0.11229074001312256, 0.07178390026092529, 0.0...\n",
       "4    0.098237  0.062555  [0.10662975162267685, 0.07581344991922379, 0.0...\n",
       "..        ...       ...                                                ...\n",
       "195  0.826296  0.181083  [0.1443796455860138, 0.052474234253168106, 0.0...\n",
       "196  0.206053  0.881988  [0.13214512169361115, 0.07988481223583221, 0.0...\n",
       "197  0.689100  0.887212  [0.15844134986400604, 0.06172007694840431, 0.0...\n",
       "198  0.089348  0.853725  [0.12560969591140747, 0.0846186950802803, 0.07...\n",
       "199  0.124021  0.197372  [0.11106044799089432, 0.07632377743721008, 0.0...\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1st parameter is a list of artifacts that we need to run inference.\n",
    "# In our example, this is simply our model\n",
    "\n",
    "# The 2nd parameter is a tuple of features, as many component as input features.\n",
    "# So in our example, it is a tuple of two components, one double for the first feature and\n",
    "# a second double for the second feature\n",
    "\n",
    "# The output must be Pytorch tensor(s). These tensors will be grouped by batches and provided to your\n",
    "# inference function\n",
    "def preprocessing_fn(\n",
    "    artifacts: artifact_type, features: Tuple[Any, ...], device: str\n",
    ") -> Tuple[torch.Tensor, ...]:\n",
    "    feature1, feature2 = features\n",
    "    return torch.Tensor([feature1]) + 1, torch.Tensor([feature2]) + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1st parameter is a list of artifacts that we need to run inference.\n",
    "# In our example, this is simply our model\n",
    "\n",
    "# The 2nd parameter is a tuple of pytorch Tensors, one tensor per input features.\n",
    "# So in our example, it is a tuple of two pytorch Tensors.\n",
    "\n",
    "# The 3rd parameter is the device name (\"cpu\", \"cuda:0\", \"cuda:1\" ...)\n",
    "def inference_fn(\n",
    "    artifacts: artifact_type, features: Tuple[torch.Tensor, ...], device: str\n",
    ") -> Tuple[Any, ...]:\n",
    "    model = artifacts\n",
    "    model.to(device)\n",
    "    feature1, feature2 = features\n",
    "    results = model(feature1, feature2)\n",
    "    return results.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions = with_inference_column_and_preprocessing(\n",
    "    df=df,\n",
    "    artifacts=serializable_model,\n",
    "    input_cols=[\"feature1\", \"feature2\"],\n",
    "    preprocessing=preprocessing_fn,\n",
    "    inference_fn=inference_fn,\n",
    "    # Our model output is an array of doubles\n",
    "    output_type=ArrayType(DoubleType()),\n",
    "    batch_size=50,\n",
    "    output_col=\"predictions\",\n",
    "    num_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g.racic/offline_inference/lib/python3.6/site-packages/pyarrow/util.py:46: FutureWarning: pyarrow.open_stream is deprecated as of 0.17.0, please use pyarrow.ipc.open_stream instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pdf = df_with_predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.585711</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>[0.2733425796031952, 0.041139136999845505, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.260878</td>\n",
       "      <td>0.160317</td>\n",
       "      <td>[0.23460765182971954, 0.049187857657670975, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979533</td>\n",
       "      <td>0.947563</td>\n",
       "      <td>[0.31539714336395264, 0.03235138952732086, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212702</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>[0.22901983559131622, 0.05043504014611244, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098237</td>\n",
       "      <td>0.062555</td>\n",
       "      <td>[0.21981926262378693, 0.053840599954128265, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.826296</td>\n",
       "      <td>0.181083</td>\n",
       "      <td>[0.2794216275215149, 0.03498457372188568, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.206053</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>[0.2501583993434906, 0.05209602043032646, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.887212</td>\n",
       "      <td>[0.28918448090553284, 0.03880692273378372, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.089348</td>\n",
       "      <td>0.853725</td>\n",
       "      <td>[0.24045471847057343, 0.05580238997936249, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.124021</td>\n",
       "      <td>0.197372</td>\n",
       "      <td>[0.22555477917194366, 0.05339846760034561, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2                                        predictions\n",
       "0    0.585711  0.622785  [0.2733425796031952, 0.041139136999845505, 0.0...\n",
       "1    0.260878  0.160317  [0.23460765182971954, 0.049187857657670975, 0....\n",
       "2    0.979533  0.947563  [0.31539714336395264, 0.03235138952732086, 0.0...\n",
       "3    0.212702  0.091959  [0.22901983559131622, 0.05043504014611244, 0.0...\n",
       "4    0.098237  0.062555  [0.21981926262378693, 0.053840599954128265, 0....\n",
       "..        ...       ...                                                ...\n",
       "195  0.826296  0.181083  [0.2794216275215149, 0.03498457372188568, 0.04...\n",
       "196  0.206053  0.881988  [0.2501583993434906, 0.05209602043032646, 0.04...\n",
       "197  0.689100  0.887212  [0.28918448090553284, 0.03880692273378372, 0.0...\n",
       "198  0.089348  0.853725  [0.24045471847057343, 0.05580238997936249, 0.0...\n",
       "199  0.124021  0.197372  [0.22555477917194366, 0.05339846760034561, 0.0...\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}